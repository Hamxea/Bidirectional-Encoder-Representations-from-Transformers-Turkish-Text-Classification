{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cmp711project_baseline_nlp_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamxea/Bidirectional-Encoder-Representations-from-Transformers-Turkish-Text-Classification/blob/main/cmp711project_baseline_nlp_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GSzSyPhso7u"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding\n",
        "import pandas as pd\n",
        "from tensorflow.python.keras.layers import GRU, Bidirectional\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Vi5Ee6vI4V",
        "outputId": "7a5301e6-19f8-4831-dc59-2d50f52dbe9f"
      },
      "source": [
        "# set environment as googledrive to folder \"resource\"\n",
        "data_path =  \"/dataset/\"\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    data_path = \"/content/drive/My Drive/dataset/\"\n",
        "\n",
        "except:\n",
        "    print(\"You are not working in Colab at the moment :(\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzBxogSxwVIz"
      },
      "source": [
        "df = pd.read_csv(data_path + 'turkish_text_data.csv', sep=';', encoding='utf-8')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHsKFVwYwX92",
        "outputId": "33e8bb9d-368c-4ff5-9b2b-5e32efa87026"
      },
      "source": [
        "df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(df.groupby('category').count())\n",
        "print(df.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           text\n",
            "category       \n",
            "dunya       677\n",
            "ekonomi     690\n",
            "kultur      567\n",
            "saglik      632\n",
            "siyaset     690\n",
            "spor        636\n",
            "teknoloji   647\n",
            "(4539, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "7r3gHyZn2zIn",
        "outputId": "9b1764e7-ad64-4bc5-c609-56bf5a49ac16"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>saglik</td>\n",
              "      <td>isınayım derken gerilmeyin sakarya_üniversites...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ekonomi</td>\n",
              "      <td>asya borsaları karışık seyretti asya borsaları...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dunya</td>\n",
              "      <td>gizli belgeleri gözardı etmeyeceğiz suriye muh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>teknoloji</td>\n",
              "      <td>chrome 24 yayınlandı google web tarayıcısının ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>demirtaş resepsiyona katılacak mı bdp eş genel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>saglik</td>\n",
              "      <td>ultrason zararsızdır denilemez art tıp merkezi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>saglik</td>\n",
              "      <td>gribe karşı limon ve bal ege_üniversitesi tıp_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>bio ajanlık aracısı haline gelebilir chp_izmir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>atalay kılıçdaroğlu risk alsın başbakan_yardım...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ekonomi</td>\n",
              "      <td>sigaracılardan yatırım tehdidi tütün ve alkol_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    category                                               text\n",
              "0     saglik  isınayım derken gerilmeyin sakarya_üniversites...\n",
              "1    ekonomi  asya borsaları karışık seyretti asya borsaları...\n",
              "2      dunya  gizli belgeleri gözardı etmeyeceğiz suriye muh...\n",
              "3  teknoloji  chrome 24 yayınlandı google web tarayıcısının ...\n",
              "4    siyaset  demirtaş resepsiyona katılacak mı bdp eş genel...\n",
              "5     saglik  ultrason zararsızdır denilemez art tıp merkezi...\n",
              "6     saglik  gribe karşı limon ve bal ege_üniversitesi tıp_...\n",
              "7    siyaset  bio ajanlık aracısı haline gelebilir chp_izmir...\n",
              "8    siyaset  atalay kılıçdaroğlu risk alsın başbakan_yardım...\n",
              "9    ekonomi  sigaracılardan yatırım tehdidi tütün ve alkol_..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p6tQVSyiBsM"
      },
      "source": [
        "### Clean the Train data, specifically remove punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MdpwwJdh_DC"
      },
      "source": [
        "import string\r\n",
        "import re\r\n",
        "\r\n",
        "totalContentCleaned = []\r\n",
        "punctDict = {}\r\n",
        "for punct in string.punctuation:\r\n",
        "    punctDict[punct] = None\r\n",
        "transString = str.maketrans(punctDict)\r\n",
        "# since we intent to remove any punctuation with ''\r\n",
        "for sen in df['text']:\r\n",
        "    \r\n",
        "    cleanedString = re.sub('[^a-zA-Z]+ ', '', sen)\r\n",
        "    \r\n",
        "    p = cleanedString.translate(transString)\r\n",
        "    totalContentCleaned.append(p)\r\n",
        "    \r\n",
        "\r\n",
        "df['text'] = totalContentCleaned"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-aCL6-awcCK",
        "outputId": "753765e4-e712-46bc-f5c6-40a559b58ac4"
      },
      "source": [
        "output_dim = df.category.unique().size\n",
        "\n",
        "target = df['category'].values.tolist()\n",
        "data = df['text'].values.tolist()\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(target)\n",
        "encoded_target = encoder.transform(target)\n",
        "# convert integers to one hot encoded vectors\n",
        "y_train = np_utils.to_categorical(encoded_target)\n",
        "X_train = data\n",
        "\n",
        "num_words= 12000\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "tokenizer.fit_on_texts(data)\n",
        "tokenizer.word_index\n",
        "print(\"Total vocab size:\", len(tokenizer.word_index))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total vocab size: 110951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-TmAibXwjRX",
        "outputId": "cc368e9a-2553-4b03-d03b-3eb08e158983"
      },
      "source": [
        "# The Tokenizer stores everything in the word_index during fit_on_texts. Then, when calling the texts_to_sequences method, only the top num_words are considered\n",
        "tokenizer.index_word[12000]\n",
        "\n",
        "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "X_train[800]\n",
        "print(X_train_tokens[800])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[383, 152, 91, 982, 67, 1651, 222, 3320, 1560, 383, 152, 4313, 3563, 522, 7304, 982, 3837, 226, 178, 278, 342, 383, 10864, 67, 982, 3505, 6550, 10152, 1, 4313, 3563, 7842, 2497, 982, 7, 2, 4046, 269, 41, 537, 4477, 4313, 3563, 522, 7304, 1808, 4296, 98, 4296, 4525, 24, 712, 1286, 2758, 982, 944, 234, 106, 169, 762, 433, 1084, 2831, 2620, 192, 1710, 3, 59, 1705, 7900, 2, 660, 91, 982, 24, 178, 404, 1416, 226, 178, 404, 54, 124, 2, 383, 9591, 5100, 11521, 4313, 3563, 522, 2, 8999, 982, 1572, 234, 226, 712, 1371, 1591, 1997, 755, 2, 2119, 1170, 2391, 383, 72, 4168, 41, 3, 7065, 24, 178, 12, 1651, 1835, 6000, 637, 129, 1394, 10035, 41, 383, 72, 982, 304, 433, 1084, 2831, 2620, 3, 1553, 2, 553, 544, 2002, 278, 383, 409, 1050, 40, 334, 2, 481, 433, 7303]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e3dUKmtwnVl",
        "outputId": "02495605-7a41-487c-f9ae-b765d1e505f6"
      },
      "source": [
        "num_tokens = [len(tokens) for tokens in X_train_tokens]\n",
        "num_tokens = np.array(num_tokens)\n",
        "\n",
        "np.mean(num_tokens)\n",
        "np.max(num_tokens)\n",
        "np.argmax(num_tokens) # index gösteriyor\n",
        "\n",
        "X_train[np.argmax(num_tokens)]\n",
        "\n",
        "# max_tokens belirleme\n",
        "max_tokens = int(np.mean(num_tokens) + (2 * np.std(num_tokens)))\n",
        "max_tokens\n",
        "\n",
        "print(\"What percentage of the total does max_token contain: %\", int(100 * np.sum(num_tokens < max_tokens) / len(num_tokens)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What percentage of the total does max_token contain: % 95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "MJGiwGZGwsIG",
        "outputId": "53e5f5cf-6847-4613-d867-f2420f6ab5ac"
      },
      "source": [
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_tokens)\n",
        "X_train_pad.shape\n",
        "\n",
        "np.array(X_train_tokens[800])\n",
        "X_train_pad[800]\n",
        "\n",
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    words = [inverse_map[token] for token in tokens if token!=0]\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "\n",
        "tokens_to_string([1,2,3,4])\n",
        "X_train[800]\n",
        "X_train_tokens[800]\n",
        "tokens_to_string(X_train_tokens[800])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'facebook ta özel mesaj önce dikkatli sosyal ağ sitesi facebook ta arkadaş listesinde olmayan birine mesaj atmak 100 dolar olabilir mi facebook haftalar önce mesaj sisteminde değişikliğe gideceğini ve arkadaş listesinde bulunmayan kişilere mesaj için bir açıklamıştı buna göre normal şartlarda arkadaş listesinde olmayan birine atılan mesajlar diğer mesajlar giderken 1 dolarlık ücret karşılığında mesaj doğrudan kişinin gelen na sistem henüz test aşamasında olduğundan birçok kullanıcı bu ancak internette dolaşan bir fotoğraf özel mesaj 1 dolar mı yoksa 100 dolar mı olduğu konusunda bir facebook kullanıcısının paylaştığı fotoğrafta arkadaş listesinde olmayan bir kullanıcıya mesaj atan kişinin 100 dolarlık ödeme yapması gerektiğine dair bir uyarı çıkıyor oysa facebook un açıklamasına göre bu bedel 1 dolar olarak dikkatli bakıldığında fotoğrafın olmadığı dikkat çekiyor uzmanlara göre facebook un mesaj sistemi henüz test aşamasında olduğundan bu tip bir sorun meydana gelmiş olabilir facebook tan konuyla ilgili herhangi bir açıklama henüz yapılmadı'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq_P3w1jwwjt"
      },
      "source": [
        "def baseline_model():\n",
        "\n",
        "    model = Sequential()\n",
        "    embedding_size = 100\n",
        "\n",
        "    model.add(Embedding(input_dim=num_words,\n",
        "                        output_dim=embedding_size,\n",
        "                        input_length=max_tokens,\n",
        "                        name='embedding_layer'))\n",
        "\n",
        "    model.add(Bidirectional(GRU(units=250, return_sequences=True)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(GRU(units=250)))\n",
        "    model.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoeoAroGzj9s",
        "outputId": "69dfca97-194e-437f-e88a-4b48465ffd0e"
      },
      "source": [
        "print(output_dim)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGY_A34mw2Cs",
        "outputId": "9c6b3ba4-da31-4b25-8a0f-8a715ae25c7a"
      },
      "source": [
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=128, verbose=1)\n",
        "kfold = KFold(n_splits=2, shuffle=True)\n",
        "results = cross_val_score(estimator, X_train_pad, y_train, cv=kfold)\n",
        "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "18/18 [==============================] - 134s 7s/step - loss: 1.9417 - accuracy: 0.1678\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 123s 7s/step - loss: 1.6998 - accuracy: 0.4132\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 125s 7s/step - loss: 0.9296 - accuracy: 0.6760\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 126s 7s/step - loss: 0.2840 - accuracy: 0.9170\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 124s 7s/step - loss: 0.0587 - accuracy: 0.9881\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 125s 7s/step - loss: 0.0143 - accuracy: 0.9984\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 124s 7s/step - loss: 0.0038 - accuracy: 0.9997\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 127s 7s/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 124s 7s/step - loss: 8.3562e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 123s 7s/step - loss: 6.0572e-04 - accuracy: 1.0000\n",
            "18/18 [==============================] - 12s 627ms/step - loss: 2.1822 - accuracy: 0.6498\n",
            "Epoch 1/10\n",
            "18/18 [==============================] - 129s 7s/step - loss: 1.9413 - accuracy: 0.1646\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 127s 7s/step - loss: 1.7097 - accuracy: 0.3724\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 125s 7s/step - loss: 1.1959 - accuracy: 0.5643\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 125s 7s/step - loss: 0.4397 - accuracy: 0.8651\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 127s 7s/step - loss: 0.0875 - accuracy: 0.9822\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 129s 7s/step - loss: 0.0185 - accuracy: 0.9981\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 126s 7s/step - loss: 0.0085 - accuracy: 0.9982\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 127s 7s/step - loss: 0.0035 - accuracy: 0.9994\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 123s 7s/step - loss: 0.0029 - accuracy: 0.9996\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 123s 7s/step - loss: 0.0013 - accuracy: 0.9999\n",
            "18/18 [==============================] - 12s 620ms/step - loss: 2.0365 - accuracy: 0.6443\n",
            "Accuracy: 64.71% (0.27%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3VwUi65ewze"
      },
      "source": [
        "precision = cross_val_score(estimator, X_train_pad, y_train, cv=kfold, scoring='precision')\r\n",
        "f1 = cross_val_score(estimator, X_train_pad, y_train, cv=kfold, scoring='f1')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jm-Q1aRMzhS"
      },
      "source": [
        "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\r\n",
        "print('Recall', np.mean(recall), recall)\r\n",
        "print('Precision', np.mean(precision), precision)\r\n",
        "print('F1', np.mean(f1), f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM5jm8WUntXn"
      },
      "source": [
        "print(\"Accuracy: %.2f%% (%.2f%%)\" % (recall.mean()*100, recall.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}