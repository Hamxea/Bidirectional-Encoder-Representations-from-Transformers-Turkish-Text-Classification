{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cmp711project_baseline_nlp_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamxea/Bidirectional-Encoder-Representations-from-Transformers-Turkish-Text-Classification/blob/main/cmp711project_baseline_nlp_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GSzSyPhso7u"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding\n",
        "import pandas as pd\n",
        "from tensorflow.python.keras.layers import GRU, Bidirectional\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Vi5Ee6vI4V",
        "outputId": "2a373558-827c-41c8-b47a-1d5e4f5ab4da"
      },
      "source": [
        "# set environment as googledrive to folder \"resource\"\n",
        "data_path =  \"/dataset/\"\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    data_path = \"/content/drive/My Drive/dataset/\"\n",
        "\n",
        "except:\n",
        "    print(\"You are not working in Colab at the moment :(\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzBxogSxwVIz"
      },
      "source": [
        "df = pd.read_csv(data_path + 'turkish_text_data.csv', sep=';', encoding='utf-8')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHsKFVwYwX92",
        "outputId": "cd4eba33-f6e7-46f4-903a-f79294e5b7a2"
      },
      "source": [
        "df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(df.groupby('category').count())\n",
        "print(df.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           text\n",
            "category       \n",
            "dunya       677\n",
            "ekonomi     690\n",
            "kultur      567\n",
            "saglik      632\n",
            "siyaset     690\n",
            "spor        636\n",
            "teknoloji   647\n",
            "(4539, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "7r3gHyZn2zIn",
        "outputId": "bbd895f5-c127-41d3-c849-b23f0e418636"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kultur</td>\n",
              "      <td>erzurum da kültür sanat erzurum devlet_tiyatro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>chp li aygün olarak gitmedim ki chp tunceli mi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>teknoloji</td>\n",
              "      <td> mwc 2013  e damga vuran ürünler huawei den ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>saglik</td>\n",
              "      <td>diş çürüğü erken doğum ve düşüğe neden olabili...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spor</td>\n",
              "      <td>şanssızlık değil hesap hatası ! başkan_aysal a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>ak_partili aksu nun hayatı belgesel oluyor ak_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>dunya</td>\n",
              "      <td>yahudilerin torunlarına vatandaşlık hakkı ispa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>dunya</td>\n",
              "      <td>afganistan  da 12 taliban öldürüldü afganista...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>saglik</td>\n",
              "      <td>canlıdan nakil kadavranın 3 5 katı internation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ekonomi</td>\n",
              "      <td>uçağa ayakta yolcu geliyor ingiliz ryanair hav...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    category                                               text\n",
              "0     kultur  erzurum da kültür sanat erzurum devlet_tiyatro...\n",
              "1    siyaset  chp li aygün olarak gitmedim ki chp tunceli mi...\n",
              "2  teknoloji   mwc 2013  e damga vuran ürünler huawei den ...\n",
              "3     saglik  diş çürüğü erken doğum ve düşüğe neden olabili...\n",
              "4       spor  şanssızlık değil hesap hatası ! başkan_aysal a...\n",
              "5    siyaset  ak_partili aksu nun hayatı belgesel oluyor ak_...\n",
              "6      dunya  yahudilerin torunlarına vatandaşlık hakkı ispa...\n",
              "7      dunya  afganistan  da 12 taliban öldürüldü afganista...\n",
              "8     saglik  canlıdan nakil kadavranın 3 5 katı internation...\n",
              "9    ekonomi  uçağa ayakta yolcu geliyor ingiliz ryanair hav..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p6tQVSyiBsM"
      },
      "source": [
        "### Clean the Train data, specifically remove punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MdpwwJdh_DC"
      },
      "source": [
        "import string\r\n",
        "import re\r\n",
        "\r\n",
        "totalContentCleaned = []\r\n",
        "punctDict = {}\r\n",
        "for punct in string.punctuation:\r\n",
        "    punctDict[punct] = None\r\n",
        "transString = str.maketrans(punctDict)\r\n",
        "# since we intent to remove any punctuation with ''\r\n",
        "for sen in df['text']:\r\n",
        "    \r\n",
        "    cleanedString = re.sub('[^a-zA-Z]+ ', '', sen)\r\n",
        "    \r\n",
        "    p = cleanedString.translate(transString)\r\n",
        "    totalContentCleaned.append(p)\r\n",
        "    \r\n",
        "\r\n",
        "df['text'] = totalContentCleaned"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-aCL6-awcCK",
        "outputId": "c264ec9e-7a72-4ac2-b844-612fa9fd22b1"
      },
      "source": [
        "output_dim = df.category.unique().size\n",
        "\n",
        "target = df['category'].values.tolist()\n",
        "data = df['text'].values.tolist()\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(target)\n",
        "encoded_target = encoder.transform(target)\n",
        "# convert integers to one hot encoded vectors\n",
        "y_train = np_utils.to_categorical(encoded_target)\n",
        "X_train = data\n",
        "\n",
        "num_words= 12000\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "tokenizer.fit_on_texts(data)\n",
        "tokenizer.word_index\n",
        "print(\"Total vocab size:\", len(tokenizer.word_index))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total vocab size: 107887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-TmAibXwjRX",
        "outputId": "86eec5d4-5b75-4146-b210-1f8b32d399a5"
      },
      "source": [
        "# The Tokenizer stores everything in the word_index during fit_on_texts. Then, when calling the texts_to_sequences method, only the top num_words are considered\n",
        "tokenizer.index_word[12000]\n",
        "\n",
        "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "X_train[800]\n",
        "print(X_train_tokens[800])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1228, 1773, 6544, 1, 2304, 6, 143, 69, 116, 7325, 6, 442, 35, 2304, 1, 6544, 6, 17, 1228, 6231, 4, 765, 1136, 73, 227, 1559, 16, 641, 511, 7, 11, 7648, 13, 4153, 604, 205, 15, 22, 7325, 17, 1228, 83, 2304, 1, 6544, 17, 1228, 6231, 4, 3115, 61, 4, 274, 6752, 2413, 9, 3406, 49, 990, 73, 227, 283, 1559, 2978, 125, 7325, 17, 1228, 6231, 758, 104, 465, 4, 2273, 2, 1228, 1773, 736, 8, 40, 12, 7283, 68, 1315, 561, 39, 1759, 1661, 6544, 17, 201, 1228, 6231, 1, 2304, 21, 32, 1228, 6231, 641, 511, 7, 11, 7648, 13, 4153, 604, 205, 379, 9344, 71, 7325, 17, 40, 781, 379, 22, 2304, 789, 135, 2, 4267, 4241, 1, 2420, 52, 64, 1559, 69, 7648, 118, 349, 7325, 11, 29, 107, 2, 379, 22, 7325, 162, 2266, 5, 565, 25, 2225, 733, 2103, 7, 1919, 8367, 604, 46, 3115, 3676, 52, 64, 1559, 118, 349, 336, 170, 30, 2, 575, 4501, 867, 2, 494, 251, 38, 1244, 9, 67, 1071, 187, 3054, 4, 1242, 1559, 3, 318, 825, 908, 86, 537, 1730, 22, 88, 1398, 623, 1267, 132, 263, 1244, 5, 9, 5, 403, 433, 157, 9, 67, 8342, 596, 174, 2977, 376, 1071, 22, 275, 1244, 217, 38, 551, 1309, 9, 5, 559, 4327, 298, 3478, 703, 9675, 60, 491, 118, 469, 2770, 5959, 104, 6801, 294, 1326, 7847, 792, 492, 94, 928, 335, 375, 2616, 2130, 3018, 578, 21, 1052, 6025, 856, 2037, 10524, 1362, 1559, 118, 4569, 131, 7130, 2, 2959, 8097, 301, 69, 94, 716, 2545, 11238, 9, 624, 367, 1723, 2, 223, 4813, 624, 11238, 9, 7104, 367, 9630, 619, 3, 375, 2616, 2994, 4647, 379, 339, 2, 405, 2789, 176, 1874, 379, 641, 3, 1148, 489, 10999, 114, 570, 8421, 559, 6717, 471, 9538, 94, 928, 4, 335, 94, 1179, 60, 9112, 596, 2901, 5471, 375, 6576, 75, 2050, 9, 67, 5050, 1689, 2008, 624, 39, 9, 7, 181, 1832, 288, 1052, 6025, 6589, 570, 699, 6296, 73, 227, 1559, 119, 119, 5, 195, 280, 22]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e3dUKmtwnVl",
        "outputId": "9edd1f49-4d3b-4281-a5dd-1b7388aab590"
      },
      "source": [
        "num_tokens = [len(tokens) for tokens in X_train_tokens]\n",
        "num_tokens = np.array(num_tokens)\n",
        "\n",
        "np.mean(num_tokens)\n",
        "np.max(num_tokens)\n",
        "np.argmax(num_tokens) # index gösteriyor\n",
        "\n",
        "X_train[np.argmax(num_tokens)]\n",
        "\n",
        "# max_tokens belirleme\n",
        "max_tokens = int(np.mean(num_tokens) + (2 * np.std(num_tokens)))\n",
        "max_tokens\n",
        "\n",
        "print(\"What percentage of the total does max_token contain: %\", int(100 * np.sum(num_tokens < max_tokens) / len(num_tokens)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What percentage of the total does max_token contain: % 95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "MJGiwGZGwsIG",
        "outputId": "f0639fea-2bc5-4de2-f82c-1e899552849c"
      },
      "source": [
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_tokens)\n",
        "X_train_pad.shape\n",
        "\n",
        "np.array(X_train_tokens[800])\n",
        "X_train_pad[800]\n",
        "\n",
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    words = [inverse_map[token] for token in tokens if token!=0]\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "\n",
        "tokens_to_string([1,2,3,4])\n",
        "X_train[800]\n",
        "X_train_tokens[800]\n",
        "tokens_to_string(X_train_tokens[800])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'halka arz vakıfbank ve ziraat \\x92 te ama yok halkbank \\x92 tan sonra ziraat ve vakıfbank \\x92 ın halka arzı da gündeme alındı başbakan yardımcısı babacan \\x93 fakat bunlar için çok acele en azından birkaç ay \\x94 dedi halkbank ın halka ardından ziraat ve vakıfbank ın halka arzı da gündemde istanbul da düzenlenen financial times türkiye zirvesi ne katılan başbakan yardımcısı ali babacan sorular üzerine halkbank ın halka arzı sonrasında diğer kamu da olası bir halka arz süreci ile ilgili olarak önümüzde iki tane konu var bunlardan birisi vakıfbank ın ikinci halka arzı ve ziraat in ilk halka arzı fakat bunlar için çok acele en azından birkaç ay lazım piyasanın önce halkbank ın ilgili etmesi lazım dedi ziraat bankası nda bir danışmanlık firması ve çalışmalarının ifade eden babacan ama acele şu anda halkbank çok yeni bunun bir lazım dedi halkbank ta hazine de kalan yüzde 51 lik pay için stratejik satışın birkaç yıl gündemde olmayacağını ifade eden babacan şu anda dünyada böyle büyük bir satın alımı yapacak bir oyuncu olmadığını söyledi şimşek türkiye ye kazandı yönelik çalışmalara da değinen babacan bu konuda çalışmaları hazır sadece uygulama bekliyoruz dedi aynı toplantıda konuşan maliye bakanı mehmet şimşek de türkiye de işaret ettiğini belirterek türkiye ye parasını herkes uzun vadede para kazandı dedi bakan şimşek şunları söyledi önümüzdeki süreçte türkiye de çalışan nüfus yine pozitif alanda kalmaya devam edecek şu an bizler nüfusun diğer yarısından tam kadınların katılımı oldukça düşük biz onlar gelecek kredi derecelendirme kuruluşu moody s in not artırımı kendisini hayal kırıklığına sorusuna babacan şu yanıtı verdi beklenti bir miktar oluşmuş olabilir ama biz hep şunu gözünde türkiye zaten yatırım yapılabilir bir ülke konumunda zaten gözünde türkiye çoktan yatırım seviyesini durumda bu kredi derecelendirme kuruluşlarının gitmesi lazım yani bir ülkenin nereye doğru vermesi lazım fakat bu kriz sonrası arkadan gelen olup anlatmaya çalışan kuruluşlar haline geldiler biz onlar da gelecek biz yapmaya devam ettikten herkes biliyor yatırımcıların kredi bakarak değil gelip türkiye ye yerinden değerlendirme yapma zaten var türkiye için 2013 yılına ait not artırımı beklentisi olup olmadığı sorulan başbakan yardımcısı babacan kendi kendi de söz burada dedi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq_P3w1jwwjt"
      },
      "source": [
        "def baseline_model():\n",
        "\n",
        "    model = Sequential()\n",
        "    embedding_size = 100\n",
        "\n",
        "    model.add(Embedding(input_dim=num_words,\n",
        "                        output_dim=embedding_size,\n",
        "                        input_length=max_tokens,\n",
        "                        name='embedding_layer'))\n",
        "\n",
        "    model.add(Bidirectional(GRU(units=250, return_sequences=True)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(GRU(units=250)))\n",
        "    model.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoeoAroGzj9s",
        "outputId": "90ea8a72-daf7-477a-c1b9-1335101c5946"
      },
      "source": [
        "print(output_dim)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGY_A34mw2Cs",
        "outputId": "c2596fee-cdfd-412c-a885-d0893213eb7f"
      },
      "source": [
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=4, batch_size=128, verbose=1)\n",
        "kfold = KFold(n_splits=2, shuffle=True)\n",
        "results = cross_val_score(estimator, X_train_pad, y_train, cv=kfold)\n",
        "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "18/18 [==============================] - 124s 6s/step - loss: 1.9437 - accuracy: 0.1531\n",
            "Epoch 2/4\n",
            "18/18 [==============================] - 117s 6s/step - loss: 1.7477 - accuracy: 0.3793\n",
            "Epoch 3/4\n",
            "18/18 [==============================] - 116s 6s/step - loss: 0.9928 - accuracy: 0.6945\n",
            "Epoch 4/4\n",
            "18/18 [==============================] - 116s 6s/step - loss: 0.3958 - accuracy: 0.8801\n",
            "18/18 [==============================] - 11s 566ms/step - loss: 1.5260 - accuracy: 0.5784\n",
            "Epoch 1/4\n",
            "18/18 [==============================] - 124s 7s/step - loss: 1.9432 - accuracy: 0.1615\n",
            "Epoch 2/4\n",
            "18/18 [==============================] - 116s 6s/step - loss: 1.6621 - accuracy: 0.3746\n",
            "Epoch 3/4\n",
            "18/18 [==============================] - 117s 6s/step - loss: 0.8458 - accuracy: 0.7358\n",
            "Epoch 4/4\n",
            "18/18 [==============================] - 116s 6s/step - loss: 0.2944 - accuracy: 0.9148\n",
            "18/18 [==============================] - 11s 570ms/step - loss: 1.6182 - accuracy: 0.6113\n",
            "Accuracy: 59.48% (1.64%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3VwUi65ewze"
      },
      "source": [
        "precision = cross_val_score(estimator, X_train_pad, y_train, cv=kfold, scoring='precision')\r\n",
        "f1 = cross_val_score(estimator, X_train_pad, y_train, cv=kfold, scoring='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jm-Q1aRMzhS"
      },
      "source": [
        "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\r\n",
        "print('Recall', np.mean(recall), recall)\r\n",
        "print('Precision', np.mean(precision), precision)\r\n",
        "print('F1', np.mean(f1), f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM5jm8WUntXn"
      },
      "source": [
        "print(\"Accuracy: %.2f%% (%.2f%%)\" % (recall.mean()*100, recall.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}